{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tests](https://github.com/jeromerg/filoc/workflows/Tests/badge.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('https://fonts.googleapis.com/css2?family=Dancing+Script:wght@400;700&display=swap');\n",
       "\n",
       ".filoc { font-size:150%; font-family: 'Dancing Script', cursive; }\n",
       "\n",
       ".highlight {\n",
       "    background-color: lemonChiffon; \n",
       "    padding:0.5em;\n",
       "    line-height:2em;\n",
       "}\n",
       ".highlight:before {\n",
       "    content: \"⮱\";\n",
       "    font-size: 2.2em;\n",
       "    position: relative;\n",
       "    bottom: -0.1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "@import url('https://fonts.googleapis.com/css2?family=Dancing+Script:wght@400;700&display=swap');\n",
    "\n",
    ".filoc { font-size:150%; font-family: 'Dancing Script', cursive; }\n",
    "\n",
    ".highlight {\n",
    "    background-color: lemonChiffon; \n",
    "    padding:0.5em;\n",
    "    line-height:2em;\n",
    "}\n",
    ".highlight:before {\n",
    "    content: \"⮱\";\n",
    "    font-size: 2.2em;\n",
    "    position: relative;\n",
    "    bottom: -0.1em;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from   torch.autograd import Variable\n",
    "import sklearn.datasets\n",
    "import keras.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span class=\"filoc\">filoc</span> in Machine Learning Workflow\n",
    "\n",
    "During a machine learning workflow, <span class=\"filoc\">filoc</span> is a simple yet powerful and scalable solution to:\n",
    "\n",
    "- Prepare the hyperparameter settings\n",
    "- Schedule and synchronize parallel simulations\n",
    "- Analyse the simulation results\n",
    "\n",
    "In comparison to existing solution like [neptune](https://neptune.ai/) and other hyperparameter tuning frameworks, it has the following advantages:\n",
    "\n",
    "- No Server: You only need a network folder or amazon s3, google cloud storage, hadoop, azure data lake\n",
    "- No Database: your files are the database\n",
    "- Framework independent: <span class=\"filoc\">filoc</span>  is not bound to any ML Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example \n",
    "\n",
    "In the following example, we will train a pytorch prediction model on the IRIS database and try to tune the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing <span class=\"filoc\">filoc</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: filoc in /home/jerom/.local/lib/python3.8/site-packages (0.0.8)\n",
      "Requirement already satisfied: parse in /home/jerom/.local/lib/python3.8/site-packages (from filoc) (1.15.0)\n",
      "Requirement already satisfied: frozendict in /home/jerom/.local/lib/python3.8/site-packages (from filoc) (1.2)\n",
      "Requirement already satisfied: orderedset in /home/jerom/.local/lib/python3.8/site-packages (from filoc) (2.0.3)\n",
      "Requirement already satisfied: fsspec in /home/jerom/.local/lib/python3.8/site-packages (from filoc) (0.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install filoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing <span class=\"filoc\">filoc</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the `filoc(...)` function. This is the most *pythonic* way to use *filoc*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filoc import filoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"highlight\">The `filoc(...)` is a factory method that returns a `FilocContract` object, which is a kind of bi-directional adapter between a DataFrame and files defined in a structured manner.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating <span class=\"filoc\">filoc</span>  instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare two filocs to read and write the following files:\n",
    "- The hyperparameter files, which contain the hyperparameters definition for a each simulation\n",
    "- The state files, used to synchronize the execution of the simulation and provide the execution state\n",
    "- The result files, which contain the exported results for each given simulation and training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_loc = filoc('./temp/iris/sim={sim:d}/hyperparameters.json', writable=True)\n",
    "state_loc       = filoc('./temp/iris/sim={sim:d}/state.json', writable=True)\n",
    "result_loc      = filoc('./temp/iris/sim={sim:d}/epoch_{epoch:d}.json', writable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"highlight\">The `filoc(...)` first argument describes the path where the files are stored. It contains format placeholders, which are part of the data that are read or written.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the model to train\n",
    "Say, we want to train the following two-layer feed-forward model with dropout to predict the iris class (3 classes) given the flower caracteristics (4 features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, layer1_size, layer2_size, dropout_prob):\n",
    "        super(Model,self).__init__()\n",
    "        self.l1      = torch.nn.Linear(4, layer1_size)\n",
    "        self.l2      = torch.nn.Linear(layer1_size, layer2_size)\n",
    "        self.l3      = torch.nn.Linear(layer2_size, 3)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_prob)\n",
    "    def forward(self, x):\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        relu    = torch.nn.ReLU()\n",
    "        y       = relu(self.l1(x))\n",
    "        y       = self.dropout(y)\n",
    "        y       = relu(self.l2(y))\n",
    "        y       = self.dropout(y)\n",
    "        y       = sigmoid(self.l3(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has 3 hyperparameters: `layer1_size`, `layer2_size`, `dropout_prob`. We will perform a simple stochastic gradient descent with constant learning rate, so we get a fourth hyperparameter: `learning_rate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the simulation hyperparameter sets\n",
    "We want to perform a basic grid search for the four hyperparameters `layer1_size`, `layer2_size`, `dropout_prob`, `learning_rate`. We build the grid manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>layer1_size</th>\n",
       "      <th>layer2_size</th>\n",
       "      <th>dropout_prob</th>\n",
       "      <th>learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sim  layer1_size  layer2_size  dropout_prob  learning_rate\n",
       "0     0            9            9          0.00          0.010\n",
       "1     1            9            9          0.00          0.001\n",
       "2     2            9            9          0.25          0.010\n",
       "3     3            9            9          0.25          0.001\n",
       "4     4            9            9          0.50          0.010\n",
       "5     5            9            9          0.50          0.001\n",
       "6     6            9            9          0.00          0.010\n",
       "7     7            9            9          0.00          0.001\n",
       "8     8            9            9          0.25          0.010\n",
       "9     9            9            9          0.25          0.001\n",
       "10   10            9            9          0.50          0.010\n",
       "11   11            9            9          0.50          0.001\n",
       "12   12            9            9          0.00          0.010\n",
       "13   13            9            9          0.00          0.001\n",
       "14   14            9            9          0.25          0.010\n",
       "15   15            9            9          0.25          0.001\n",
       "16   16            9            9          0.50          0.010\n",
       "17   17            9            9          0.50          0.001\n",
       "18   18           27           27          0.00          0.010\n",
       "19   19           27           27          0.00          0.001\n",
       "20   20           27           27          0.25          0.010\n",
       "21   21           27           27          0.25          0.001\n",
       "22   22           27           27          0.50          0.010\n",
       "23   23           27           27          0.50          0.001\n",
       "24   24           27           27          0.00          0.010\n",
       "25   25           27           27          0.00          0.001\n",
       "26   26           27           27          0.25          0.010\n",
       "27   27           27           27          0.25          0.001\n",
       "28   28           27           27          0.50          0.010\n",
       "29   29           27           27          0.50          0.001\n",
       "30   30           27           27          0.00          0.010\n",
       "31   31           27           27          0.00          0.001\n",
       "32   32           27           27          0.25          0.010\n",
       "33   33           27           27          0.25          0.001\n",
       "34   34           27           27          0.50          0.010\n",
       "35   35           27           27          0.50          0.001\n",
       "36   36           81           81          0.00          0.010\n",
       "37   37           81           81          0.00          0.001\n",
       "38   38           81           81          0.25          0.010\n",
       "39   39           81           81          0.25          0.001\n",
       "40   40           81           81          0.50          0.010\n",
       "41   41           81           81          0.50          0.001\n",
       "42   42           81           81          0.00          0.010\n",
       "43   43           81           81          0.00          0.001\n",
       "44   44           81           81          0.25          0.010\n",
       "45   45           81           81          0.25          0.001\n",
       "46   46           81           81          0.50          0.010\n",
       "47   47           81           81          0.50          0.001\n",
       "48   48           81           81          0.00          0.010\n",
       "49   49           81           81          0.00          0.001\n",
       "50   50           81           81          0.25          0.010\n",
       "51   51           81           81          0.25          0.001\n",
       "52   52           81           81          0.50          0.010\n",
       "53   53           81           81          0.50          0.001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. generate hyperparameter set candidates\n",
    "layer1_sizes   = [9, 27, 81]\n",
    "layer2_sizes   = [9, 27, 81]\n",
    "dropout_probs  = [0, 0.25, 0.5]\n",
    "learning_rates = [0.01, 0.001]\n",
    "\n",
    "sims_hyperparameters = []\n",
    "for sim, (layer1_size, layer2_size, dropout_prob, learning_rate) in enumerate(itertools.product(layer1_sizes, layer2_sizes, dropout_probs, learning_rates)):\n",
    "    sims_hyperparameters.append({\n",
    "        'sim'           : sim, \n",
    "        'layer1_size'   : layer1_size, \n",
    "        'layer2_size'   : layer1_size, \n",
    "        'dropout_prob'  : dropout_prob, \n",
    "        'learning_rate' : learning_rate\n",
    "    })\n",
    "pd.DataFrame(sims_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the simulation hyperparameter sets\n",
    "\n",
    "Now, we need to save each simulation hyperparameter set into a hyperparameter file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_loc.write_contents(sims_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"highlight\">`FilocContract.write_contents(...)` created all the 53 hyperparameters files at once! It replaced the `sim` variable defined in the filoc path `./temp/iris/sim={sim:d}/hyperparameters.json`  by the `sim` value defined in the `sim_hyperparameters` list, then created the files and filled them with the remaining variables of the object.\n",
    "</span>\n",
    "\n",
    "Let's check the created files with the terminal `ls` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./temp/iris/sim=0/hyperparameters.json'\n",
      "'./temp/iris/sim=10/hyperparameters.json'\n",
      "'./temp/iris/sim=11/hyperparameters.json'\n",
      "'./temp/iris/sim=12/hyperparameters.json'\n",
      "'./temp/iris/sim=13/hyperparameters.json'\n",
      "'./temp/iris/sim=14/hyperparameters.json'\n",
      "'./temp/iris/sim=15/hyperparameters.json'\n",
      "'./temp/iris/sim=16/hyperparameters.json'\n",
      "'./temp/iris/sim=17/hyperparameters.json'\n",
      "'./temp/iris/sim=18/hyperparameters.json'\n",
      "'./temp/iris/sim=19/hyperparameters.json'\n",
      "'./temp/iris/sim=1/hyperparameters.json'\n",
      "'./temp/iris/sim=20/hyperparameters.json'\n",
      "'./temp/iris/sim=21/hyperparameters.json'\n",
      "'./temp/iris/sim=22/hyperparameters.json'\n",
      "'./temp/iris/sim=23/hyperparameters.json'\n",
      "'./temp/iris/sim=24/hyperparameters.json'\n",
      "'./temp/iris/sim=25/hyperparameters.json'\n",
      "'./temp/iris/sim=26/hyperparameters.json'\n",
      "'./temp/iris/sim=27/hyperparameters.json'\n",
      "'./temp/iris/sim=28/hyperparameters.json'\n",
      "'./temp/iris/sim=29/hyperparameters.json'\n",
      "'./temp/iris/sim=2/hyperparameters.json'\n",
      "'./temp/iris/sim=30/hyperparameters.json'\n",
      "'./temp/iris/sim=31/hyperparameters.json'\n",
      "'./temp/iris/sim=32/hyperparameters.json'\n",
      "'./temp/iris/sim=33/hyperparameters.json'\n",
      "'./temp/iris/sim=34/hyperparameters.json'\n",
      "'./temp/iris/sim=35/hyperparameters.json'\n",
      "'./temp/iris/sim=36/hyperparameters.json'\n",
      "'./temp/iris/sim=37/hyperparameters.json'\n",
      "'./temp/iris/sim=38/hyperparameters.json'\n",
      "'./temp/iris/sim=39/hyperparameters.json'\n",
      "'./temp/iris/sim=3/hyperparameters.json'\n",
      "'./temp/iris/sim=40/hyperparameters.json'\n",
      "'./temp/iris/sim=41/hyperparameters.json'\n",
      "'./temp/iris/sim=42/hyperparameters.json'\n",
      "'./temp/iris/sim=43/hyperparameters.json'\n",
      "'./temp/iris/sim=44/hyperparameters.json'\n",
      "'./temp/iris/sim=45/hyperparameters.json'\n",
      "'./temp/iris/sim=46/hyperparameters.json'\n",
      "'./temp/iris/sim=47/hyperparameters.json'\n",
      "'./temp/iris/sim=48/hyperparameters.json'\n",
      "'./temp/iris/sim=49/hyperparameters.json'\n",
      "'./temp/iris/sim=4/hyperparameters.json'\n",
      "'./temp/iris/sim=50/hyperparameters.json'\n",
      "'./temp/iris/sim=51/hyperparameters.json'\n",
      "'./temp/iris/sim=52/hyperparameters.json'\n",
      "'./temp/iris/sim=53/hyperparameters.json'\n",
      "'./temp/iris/sim=5/hyperparameters.json'\n",
      "'./temp/iris/sim=6/hyperparameters.json'\n",
      "'./temp/iris/sim=7/hyperparameters.json'\n",
      "'./temp/iris/sim=8/hyperparameters.json'\n",
      "'./temp/iris/sim=9/hyperparameters.json'\n"
     ]
    }
   ],
   "source": [
    "! ls ./temp/iris/*/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a single file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"layer1_size\": 27,\n",
      "  \"layer2_size\": 27,\n",
      "  \"dropout_prob\": 0.5,\n",
      "  \"learning_rate\": 0.001\n",
      "}"
     ]
    }
   ],
   "source": [
    "! cat ./temp/iris/sim=23/hyperparameters.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the state file\n",
    "\n",
    "We want to track the training state, in order to parallelize the simulation trainings and to keep the overview on which simulation has been performed, which not.\n",
    "We need 4 states:\n",
    "\n",
    "- `init` : simulation is ready to be executed\n",
    "- `running` : simulation is currently executed\n",
    "- `failed` : simulation has failed and stopped\n",
    "- `succeeded` : simulation has ended properly\n",
    "\n",
    "We prepare the state file for each simulation, in the same way as we previously prepared the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_states = []\n",
    "for hyp in sims_hyperparameters:\n",
    "    sim_states.append({\n",
    "        'sim'   : hyp['sim'], \n",
    "        'state' : 'init'\n",
    "    })\n",
    "\n",
    "with state_loc.lock():\n",
    "    state_loc.write_contents(sim_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"highlight\">We locked the access to all state files within a `state_loc.lock()` with-block. It ensures that no other process currently read from nor write to the state files.\n",
    "</span>\n",
    "\n",
    "The locking implementation is relative simple and works only against other use of `state_loc.lock()` in other threads/processes/machines. But it is efficient enough to ensure the serialization of read and write accesses of concurrent sensitive files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the simulations\n",
    "\n",
    "Now we are ready to perform the simulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X=torch.Size([100, 4]), valid_X=torch.Size([50, 4]), train_y=torch.Size([100, 3]), valid_y=torch.Size([50, 3])\n"
     ]
    }
   ],
   "source": [
    "# load training and validation dataset\n",
    "iris    = sklearn.datasets.load_iris()\n",
    "indexes = list(range(len(iris.data)))\n",
    "np.random.shuffle(indexes)\n",
    "all_X   = torch.from_numpy(iris.data).type(torch.DoubleTensor)\n",
    "all_y   = torch.from_numpy(keras.utils.to_categorical(iris.target)).type(torch.DoubleTensor)\n",
    "train_X = Variable(all_X[indexes[:100]])\n",
    "train_y = Variable(all_y[indexes[:100]])\n",
    "valid_X = Variable(all_X[indexes[100:150]])\n",
    "valid_y = Variable(all_y[indexes[100:150]])\n",
    "\n",
    "#print(f'train_X={train_X.shape}, valid_X={valid_X.shape}, train_y={train_y.shape}, valid_y={valid_y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 0.03168233444636589, 'train_acc': 0.98, 'valid_loss': 0.049163009214078436, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.03522447352570929, 'train_acc': 0.99, 'valid_loss': 0.06771883435554166, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.021743806819387975, 'train_acc': 0.99, 'valid_loss': 0.1412255768291411, 'valid_acc': 0.92}\n",
      "{'train_loss': 0.039445649824771306, 'train_acc': 0.99, 'valid_loss': 0.1785807651839132, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.03744309580910632, 'train_acc': 0.98, 'valid_loss': 0.12659499807062835, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.027803571842636956, 'train_acc': 0.98, 'valid_loss': 0.11549584064673572, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.04293671748140396, 'train_acc': 0.97, 'valid_loss': 0.11885397725995524, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.017730040639064654, 'train_acc': 1.0, 'valid_loss': 0.12049402162170611, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.024299737818919174, 'train_acc': 1.0, 'valid_loss': 0.11415369453383738, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.036619369193951194, 'train_acc': 0.99, 'valid_loss': 0.054409181159117685, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.02600275368473521, 'train_acc': 1.0, 'valid_loss': 0.13531728765642317, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.0361077566375484, 'train_acc': 0.98, 'valid_loss': 0.09056123891132409, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.022180789400407443, 'train_acc': 1.0, 'valid_loss': 0.085043928450555, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.021001396618760836, 'train_acc': 0.99, 'valid_loss': 0.17301691950652678, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.02365862787509417, 'train_acc': 0.99, 'valid_loss': 0.14054877322398998, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.01949802435826252, 'train_acc': 0.99, 'valid_loss': 0.07419688873457356, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.02883526630105239, 'train_acc': 0.99, 'valid_loss': 0.1189915894588314, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.06706252599716664, 'train_acc': 0.97, 'valid_loss': 0.10240072593528694, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.034609690817821705, 'train_acc': 0.99, 'valid_loss': 0.0915231719945878, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.03918353653928996, 'train_acc': 0.99, 'valid_loss': 0.12254842706018029, 'valid_acc': 0.92}\n",
      "{'train_loss': 0.03487675349108049, 'train_acc': 0.97, 'valid_loss': 0.09699471579582068, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.04627646456593695, 'train_acc': 0.97, 'valid_loss': 0.12012164918340411, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.031122030382150973, 'train_acc': 0.99, 'valid_loss': 0.11372052435748302, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.03950945723445341, 'train_acc': 0.97, 'valid_loss': 0.10284357968688465, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.017108017796952407, 'train_acc': 1.0, 'valid_loss': 0.1222038083879549, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.041693887146098986, 'train_acc': 0.98, 'valid_loss': 0.05961655878307958, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.04937952248227539, 'train_acc': 0.97, 'valid_loss': 0.06934351498284587, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.04173890323400827, 'train_acc': 0.98, 'valid_loss': 0.09976637876339446, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.033387111721317426, 'train_acc': 0.99, 'valid_loss': 0.13308737032047901, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.04373299580012896, 'train_acc': 0.96, 'valid_loss': 0.10605512189629134, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.02906319784700375, 'train_acc': 0.98, 'valid_loss': 0.07859336923143102, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.02569675116453915, 'train_acc': 0.99, 'valid_loss': 0.12226267392971951, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.017117195010567122, 'train_acc': 0.99, 'valid_loss': 0.11610918483838704, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.03299337446611251, 'train_acc': 0.98, 'valid_loss': 0.12325668061486207, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.030071676843232614, 'train_acc': 0.99, 'valid_loss': 0.11725683918026161, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.017390131495303218, 'train_acc': 1.0, 'valid_loss': 0.10935314595610161, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.04207949333774822, 'train_acc': 0.99, 'valid_loss': 0.09135198564713869, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.02235904881276332, 'train_acc': 1.0, 'valid_loss': 0.1316906552445207, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.018736945788991516, 'train_acc': 1.0, 'valid_loss': 0.12290727577190125, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.029205666553156148, 'train_acc': 0.99, 'valid_loss': 0.09129511988816097, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.039993197007836497, 'train_acc': 0.99, 'valid_loss': 0.08805140118097037, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.02677264966255682, 'train_acc': 0.98, 'valid_loss': 0.10579397444139041, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.011545859286269942, 'train_acc': 1.0, 'valid_loss': 0.08212697140488576, 'valid_acc': 0.92}\n",
      "{'train_loss': 0.023192156400869107, 'train_acc': 1.0, 'valid_loss': 0.09166319893451204, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.03385128531684752, 'train_acc': 0.99, 'valid_loss': 0.11720956838298074, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.016441844356987128, 'train_acc': 1.0, 'valid_loss': 0.17047004612809188, 'valid_acc': 0.92}\n",
      "{'train_loss': 0.012288960389389375, 'train_acc': 1.0, 'valid_loss': 0.10726423540566815, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.039189620775983564, 'train_acc': 0.98, 'valid_loss': 0.12922848049671343, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.014731769846681357, 'train_acc': 1.0, 'valid_loss': 0.08783966770140975, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.017913076797428536, 'train_acc': 0.99, 'valid_loss': 0.09352707885474495, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.05012157777434599, 'train_acc': 0.96, 'valid_loss': 0.08978781848116822, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.039497604780090366, 'train_acc': 0.98, 'valid_loss': 0.13620907995653211, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.06264829177417942, 'train_acc': 0.97, 'valid_loss': 0.05271619527149535, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.037747604519440175, 'train_acc': 0.98, 'valid_loss': 0.15126667574907507, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.018466041486330704, 'train_acc': 0.99, 'valid_loss': 0.10301493561573558, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.02642497720564795, 'train_acc': 0.99, 'valid_loss': 0.049190448069587694, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.029850838212170584, 'train_acc': 0.99, 'valid_loss': 0.17528837536813285, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.02949841336825732, 'train_acc': 0.99, 'valid_loss': 0.09291467923013687, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.03996860577868159, 'train_acc': 0.98, 'valid_loss': 0.12514966797753344, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.05407315116455041, 'train_acc': 0.98, 'valid_loss': 0.08913981273094587, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.03203211678334009, 'train_acc': 0.99, 'valid_loss': 0.10567555189264563, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.018181132696147045, 'train_acc': 0.99, 'valid_loss': 0.07890889036245104, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.010726978496398035, 'train_acc': 1.0, 'valid_loss': 0.17839584593260438, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.04510108650994341, 'train_acc': 0.96, 'valid_loss': 0.1202342595329205, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.02660736397146764, 'train_acc': 0.99, 'valid_loss': 0.07599826708091596, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.023063393982507386, 'train_acc': 0.99, 'valid_loss': 0.07894374361846832, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.03044061606175185, 'train_acc': 0.99, 'valid_loss': 0.1019249932552485, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.02955567660632681, 'train_acc': 0.99, 'valid_loss': 0.08298450888989695, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.0287156928692925, 'train_acc': 0.99, 'valid_loss': 0.16040064315770788, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.03044077158993984, 'train_acc': 0.98, 'valid_loss': 0.10388337910334115, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.01922409708011772, 'train_acc': 1.0, 'valid_loss': 0.07617800972631307, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.021112988314896195, 'train_acc': 0.98, 'valid_loss': 0.10670477514005294, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.02134443923643371, 'train_acc': 0.99, 'valid_loss': 0.10218681510374536, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.027526748653597714, 'train_acc': 0.98, 'valid_loss': 0.14559504163412335, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.023047501183909833, 'train_acc': 0.99, 'valid_loss': 0.12247200765456291, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.02692354274100723, 'train_acc': 0.99, 'valid_loss': 0.12155549080811107, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.012742604970381922, 'train_acc': 1.0, 'valid_loss': 0.12517107369189306, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.01892080067217379, 'train_acc': 1.0, 'valid_loss': 0.07890507907783219, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.019472730581129357, 'train_acc': 1.0, 'valid_loss': 0.09417256167300571, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.03110034677862125, 'train_acc': 0.99, 'valid_loss': 0.12874782380633631, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.03910708540063086, 'train_acc': 0.98, 'valid_loss': 0.08053548942982852, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.017542077547102474, 'train_acc': 1.0, 'valid_loss': 0.18695579978929594, 'valid_acc': 0.92}\n",
      "{'train_loss': 0.027975346287769275, 'train_acc': 0.99, 'valid_loss': 0.06494155921247384, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.024700636248273577, 'train_acc': 0.98, 'valid_loss': 0.04843650062423574, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.02346244330337484, 'train_acc': 0.98, 'valid_loss': 0.06188145557562514, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.04185761605620451, 'train_acc': 0.98, 'valid_loss': 0.11699699037106413, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.04424615060360964, 'train_acc': 0.98, 'valid_loss': 0.12209323943015953, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.019552712317525534, 'train_acc': 0.99, 'valid_loss': 0.07805320488258871, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.019745586935463032, 'train_acc': 0.99, 'valid_loss': 0.09549549340941642, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.018318973759471315, 'train_acc': 0.99, 'valid_loss': 0.05881674152257411, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.03323158365748712, 'train_acc': 0.99, 'valid_loss': 0.06569662994700046, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.018306226876683852, 'train_acc': 0.99, 'valid_loss': 0.1084066965099179, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.019246257486463268, 'train_acc': 1.0, 'valid_loss': 0.0856132090906671, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.01894968772139203, 'train_acc': 0.99, 'valid_loss': 0.1480661918004005, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.03894704670232487, 'train_acc': 0.97, 'valid_loss': 0.10935303181659405, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.023902523323017868, 'train_acc': 0.99, 'valid_loss': 0.09437569848228695, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.024330985511126915, 'train_acc': 0.99, 'valid_loss': 0.11042523548108701, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.028785516554131294, 'train_acc': 0.98, 'valid_loss': 0.12752551396471384, 'valid_acc': 0.92}\n",
      "{'train_loss': 0.04815928847774489, 'train_acc': 0.96, 'valid_loss': 0.14480366581820528, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.01385644099036934, 'train_acc': 0.99, 'valid_loss': 0.10324232706671571, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.03634029036671418, 'train_acc': 0.99, 'valid_loss': 0.0981049426678084, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.017939573809710382, 'train_acc': 1.0, 'valid_loss': 0.09843250476606233, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.014454527295216565, 'train_acc': 1.0, 'valid_loss': 0.1102653797852851, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.02118343462057667, 'train_acc': 1.0, 'valid_loss': 0.08501784038626295, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.022777479714384815, 'train_acc': 0.98, 'valid_loss': 0.12249710920555866, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.01212240429893733, 'train_acc': 1.0, 'valid_loss': 0.13026883948933246, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.014350017215387053, 'train_acc': 0.99, 'valid_loss': 0.043509526103025054, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.02761887928994879, 'train_acc': 0.99, 'valid_loss': 0.1651413285597023, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.015074521174283317, 'train_acc': 1.0, 'valid_loss': 0.09760957384654409, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.0264341876739554, 'train_acc': 0.98, 'valid_loss': 0.0761031690035303, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.030236653086513854, 'train_acc': 0.99, 'valid_loss': 0.11873655397662052, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.024250353787028514, 'train_acc': 0.99, 'valid_loss': 0.0877467757383433, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.011228905210422965, 'train_acc': 1.0, 'valid_loss': 0.08977377517744543, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.034656127062146484, 'train_acc': 0.99, 'valid_loss': 0.13479647981268078, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.013495837547806942, 'train_acc': 1.0, 'valid_loss': 0.11454519254669782, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.030054533629494517, 'train_acc': 0.97, 'valid_loss': 0.14061537609867986, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.017681964457112592, 'train_acc': 0.98, 'valid_loss': 0.07874931475394117, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.02606370048210256, 'train_acc': 0.98, 'valid_loss': 0.18455102009865282, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.024173218111264036, 'train_acc': 0.99, 'valid_loss': 0.08015757769056292, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.013188585548728742, 'train_acc': 0.99, 'valid_loss': 0.11345591246787123, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.04665699387545868, 'train_acc': 0.99, 'valid_loss': 0.16411650326751698, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.02129753515490113, 'train_acc': 0.99, 'valid_loss': 0.06485176888590113, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.022018785042168317, 'train_acc': 0.99, 'valid_loss': 0.04752559021195658, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.01105728816671633, 'train_acc': 1.0, 'valid_loss': 0.1686479419177001, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.021115459820937328, 'train_acc': 0.99, 'valid_loss': 0.0784106574149693, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.03407676348929473, 'train_acc': 0.99, 'valid_loss': 0.11208614505153082, 'valid_acc': 0.92}\n",
      "{'train_loss': 0.010236411577912484, 'train_acc': 1.0, 'valid_loss': 0.1373531386067676, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.017552871522093928, 'train_acc': 0.99, 'valid_loss': 0.08178760921620076, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.01922852536959063, 'train_acc': 1.0, 'valid_loss': 0.06027486840531067, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.026372920587828762, 'train_acc': 0.99, 'valid_loss': 0.12655964366675093, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.0159853159542835, 'train_acc': 1.0, 'valid_loss': 0.1297988126505517, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.042694708668153405, 'train_acc': 0.98, 'valid_loss': 0.09458581670942603, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.01646870496134531, 'train_acc': 1.0, 'valid_loss': 0.08308495906399595, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.013937525314591819, 'train_acc': 0.99, 'valid_loss': 0.1811510237798185, 'valid_acc': 0.9}\n",
      "{'train_loss': 0.03028496453418733, 'train_acc': 0.98, 'valid_loss': 0.09497891885385525, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.012267049689187782, 'train_acc': 1.0, 'valid_loss': 0.1412337424138144, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.012598937162512801, 'train_acc': 1.0, 'valid_loss': 0.14991289276132402, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.009508485890029277, 'train_acc': 1.0, 'valid_loss': 0.07002100154586158, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.022455297866685663, 'train_acc': 0.98, 'valid_loss': 0.08923029175493975, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.022877968345780847, 'train_acc': 0.98, 'valid_loss': 0.11334006711602655, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.02039921899707409, 'train_acc': 1.0, 'valid_loss': 0.07682941571842186, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.018207955441236465, 'train_acc': 0.99, 'valid_loss': 0.12816513186284811, 'valid_acc': 0.92}\n",
      "{'train_loss': 0.016506017700031805, 'train_acc': 1.0, 'valid_loss': 0.11647988514053295, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.01154577258016064, 'train_acc': 1.0, 'valid_loss': 0.13507851198070756, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.020252129481750425, 'train_acc': 1.0, 'valid_loss': 0.18956941664518445, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.012695477071333172, 'train_acc': 1.0, 'valid_loss': 0.13286904462277005, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.023499539329170968, 'train_acc': 0.99, 'valid_loss': 0.09407280382397457, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.024696208088908277, 'train_acc': 0.99, 'valid_loss': 0.07454794960696005, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.02064681284751334, 'train_acc': 0.99, 'valid_loss': 0.11946207502771959, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.013908861127629181, 'train_acc': 1.0, 'valid_loss': 0.12499029832248498, 'valid_acc': 0.9}\n",
      "{'train_loss': 0.02461309684053046, 'train_acc': 1.0, 'valid_loss': 0.07852257876220434, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.010765596211416347, 'train_acc': 1.0, 'valid_loss': 0.07481114186160831, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.015243021624753519, 'train_acc': 1.0, 'valid_loss': 0.09799181775060066, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.008404737123182283, 'train_acc': 1.0, 'valid_loss': 0.060825591967060146, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.015633838020257487, 'train_acc': 1.0, 'valid_loss': 0.1450345144707491, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.015085620917722498, 'train_acc': 1.0, 'valid_loss': 0.180829372128357, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.017914087906022193, 'train_acc': 1.0, 'valid_loss': 0.11730478114161637, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.01842830709574738, 'train_acc': 1.0, 'valid_loss': 0.11934465079573797, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.012379936466194284, 'train_acc': 1.0, 'valid_loss': 0.05881541572525702, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.048197390306231734, 'train_acc': 0.97, 'valid_loss': 0.0917090118400826, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.021025840331098635, 'train_acc': 0.99, 'valid_loss': 0.10393230593872259, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.015273148280480884, 'train_acc': 0.99, 'valid_loss': 0.07617819271495584, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.012678425201172214, 'train_acc': 1.0, 'valid_loss': 0.04490681485709203, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.013496959204370648, 'train_acc': 1.0, 'valid_loss': 0.07104050987265983, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.016724759144547297, 'train_acc': 1.0, 'valid_loss': 0.1125932810193755, 'valid_acc': 0.92}\n",
      "{'train_loss': 0.021229756439404036, 'train_acc': 0.98, 'valid_loss': 0.06593835779518657, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.019909897988096458, 'train_acc': 0.99, 'valid_loss': 0.08790998987477447, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.03405984904177618, 'train_acc': 0.98, 'valid_loss': 0.0787030997918916, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.01706881838920296, 'train_acc': 0.99, 'valid_loss': 0.11485141516083866, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.02574154652427579, 'train_acc': 0.98, 'valid_loss': 0.12002413767769692, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.023861866778818703, 'train_acc': 0.99, 'valid_loss': 0.06312716905384558, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.014297306426368825, 'train_acc': 1.0, 'valid_loss': 0.06796089897296233, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.018603068390014185, 'train_acc': 1.0, 'valid_loss': 0.1776775213114645, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.009575784441743255, 'train_acc': 1.0, 'valid_loss': 0.10233757895461389, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.01878263321462431, 'train_acc': 1.0, 'valid_loss': 0.08562358923901749, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.01341292860258078, 'train_acc': 1.0, 'valid_loss': 0.09137379803434159, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.01374027305386363, 'train_acc': 1.0, 'valid_loss': 0.09614716307827968, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.019599350839706885, 'train_acc': 1.0, 'valid_loss': 0.16531130770322605, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.032852062810247364, 'train_acc': 0.98, 'valid_loss': 0.0685064619922584, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.02570176770128772, 'train_acc': 0.98, 'valid_loss': 0.11609899484895074, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.04796298694904037, 'train_acc': 0.97, 'valid_loss': 0.1246221335867864, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.014490519872002366, 'train_acc': 1.0, 'valid_loss': 0.07732427491551412, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.01815422586548424, 'train_acc': 0.99, 'valid_loss': 0.08719946065637461, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.016350694792846118, 'train_acc': 1.0, 'valid_loss': 0.1486522977543639, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.018375982834592384, 'train_acc': 0.99, 'valid_loss': 0.09791524350580602, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.023881083481408164, 'train_acc': 0.99, 'valid_loss': 0.09184179199388123, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.012057231838035183, 'train_acc': 1.0, 'valid_loss': 0.08457432937734936, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.012558994663708207, 'train_acc': 1.0, 'valid_loss': 0.11642902129128445, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.023277956750893856, 'train_acc': 0.99, 'valid_loss': 0.19209155500621486, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.019209555024587103, 'train_acc': 0.99, 'valid_loss': 0.07848930118101308, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.04392927405606934, 'train_acc': 0.96, 'valid_loss': 0.08912018850907916, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.011344886470145754, 'train_acc': 1.0, 'valid_loss': 0.07428890972954036, 'valid_acc': 0.98}\n",
      "{'train_loss': 0.02108662241434365, 'train_acc': 0.99, 'valid_loss': 0.08253201665601431, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.028100217860556456, 'train_acc': 0.98, 'valid_loss': 0.19134837472812605, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.013503549930246734, 'train_acc': 1.0, 'valid_loss': 0.10240980726802854, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.018069628668028432, 'train_acc': 0.99, 'valid_loss': 0.18206858021275732, 'valid_acc': 0.94}\n",
      "{'train_loss': 0.014746652250194362, 'train_acc': 0.99, 'valid_loss': 0.11593390873256987, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.012165109581157412, 'train_acc': 1.0, 'valid_loss': 0.13257449128030907, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.01159534171348399, 'train_acc': 1.0, 'valid_loss': 0.14194431348697004, 'valid_acc': 0.96}\n",
      "{'train_loss': 0.040770458795416506, 'train_acc': 0.99, 'valid_loss': 0.10240940366322408, 'valid_acc': 0.94}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "layer1_size   = 100\n",
    "layer2_size   = 100\n",
    "learning_rate = 0.01\n",
    "dropout_prob  = 0.5\n",
    "\n",
    "model    = Model(layer1_size, layer2_size, dropout_prob).double()\n",
    "\n",
    "loss_fun = torch.nn.BCELoss(size_average=True)\n",
    "opt      = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(20000):\n",
    "    train_y_pred = model(train_X)\n",
    "    train_loss   = loss_fun(train_y_pred, train_y)    \n",
    "    train_acc    = (torch.argmax(train_y_pred, dim=1) == torch.argmax(train_y, dim=1)).sum().item() / len(train_y)\n",
    "    opt.zero_grad()\n",
    "    train_loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        valid_y_pred = model(valid_X)\n",
    "        valid_loss   = loss_fun(valid_y_pred, valid_y)    \n",
    "        valid_acc    = (torch.argmax(valid_y_pred, dim=1) == torch.argmax(valid_y, dim=1)).sum().item() / len(valid_y)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print({\n",
    "            'train_loss' : train_loss.item(),\n",
    "            'train_acc'  : train_acc,\n",
    "            'valid_loss' : valid_loss.item(),\n",
    "            'valid_acc'  : valid_acc,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./temp/iris/candid=18/epoch_0.json'\n",
      "'./temp/iris/candid=18/epoch_10.json'\n",
      "'./temp/iris/candid=18/epoch_11.json'\n",
      "'./temp/iris/candid=18/epoch_12.json'\n",
      "'./temp/iris/candid=18/epoch_13.json'\n",
      "'./temp/iris/candid=18/epoch_14.json'\n",
      "'./temp/iris/candid=18/epoch_15.json'\n",
      "'./temp/iris/candid=18/epoch_16.json'\n",
      "'./temp/iris/candid=18/epoch_17.json'\n",
      "'./temp/iris/candid=18/epoch_18.json'\n",
      "'./temp/iris/candid=18/epoch_19.json'\n",
      "'./temp/iris/candid=18/epoch_1.json'\n",
      "'./temp/iris/candid=18/epoch_2.json'\n",
      "'./temp/iris/candid=18/epoch_3.json'\n",
      "'./temp/iris/candid=18/epoch_4.json'\n",
      "'./temp/iris/candid=18/epoch_5.json'\n",
      "'./temp/iris/candid=18/epoch_6.json'\n",
      "'./temp/iris/candid=18/epoch_7.json'\n",
      "'./temp/iris/candid=18/epoch_8.json'\n",
      "'./temp/iris/candid=18/epoch_9.json'\n",
      "'./temp/iris/candid=18/hyperparameters.json'\n"
     ]
    }
   ],
   "source": [
    "! ls ./temp/iris/candid=18/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"score\": 0.86\n",
      "}"
     ]
    }
   ],
   "source": [
    "! cat ./temp/iris/candid=18/epoch_19.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we perform trainings for each simulation and stored the result for each epoch. Now we can start the analysis. First we need to join together the hyperparameters with the results. For that use again the `filoc` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index.epoch</th>\n",
       "      <th>index.candid</th>\n",
       "      <th>hyp.kernel</th>\n",
       "      <th>hyp.C</th>\n",
       "      <th>hyp.degree</th>\n",
       "      <th>res.score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>19</td>\n",
       "      <td>139</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>19</td>\n",
       "      <td>140</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>19</td>\n",
       "      <td>141</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>19</td>\n",
       "      <td>142</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>5</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>19</td>\n",
       "      <td>143</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>6</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2880 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index.epoch  index.candid hyp.kernel    hyp.C  hyp.degree  res.score\n",
       "0               0             0     linear     0.01           1       0.88\n",
       "1               0             1     linear     0.01           2       0.88\n",
       "2               0             2     linear     0.01           3       0.88\n",
       "3               0             3     linear     0.01           4       0.88\n",
       "4               0             4     linear     0.01           5       0.88\n",
       "...           ...           ...        ...      ...         ...        ...\n",
       "2875           19           139    sigmoid  1000.00           2       0.86\n",
       "2876           19           140    sigmoid  1000.00           3       0.86\n",
       "2877           19           141    sigmoid  1000.00           4       0.86\n",
       "2878           19           142    sigmoid  1000.00           5       0.86\n",
       "2879           19           143    sigmoid  1000.00           6       0.86\n",
       "\n",
       "[2880 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc = filoc( {'hyp' : hyperparams_loc, 'res' : result_loc}, frontend='pandas', cache_locpath='temp/cache/iris/cache.bin')\n",
    "df  = loc.read_contents()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index.epoch</th>\n",
       "      <th>index.candid</th>\n",
       "      <th>hyp.C</th>\n",
       "      <th>hyp.degree</th>\n",
       "      <th>res.score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2880.000000</td>\n",
       "      <td>2880.000000</td>\n",
       "      <td>2880.000000</td>\n",
       "      <td>2880.000000</td>\n",
       "      <td>2880.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.500000</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>185.185000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.858056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.767283</td>\n",
       "      <td>41.575436</td>\n",
       "      <td>366.201103</td>\n",
       "      <td>1.708122</td>\n",
       "      <td>0.042975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.750000</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.500000</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.250000</td>\n",
       "      <td>107.250000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index.epoch  index.candid        hyp.C   hyp.degree    res.score\n",
       "count  2880.000000   2880.000000  2880.000000  2880.000000  2880.000000\n",
       "mean      9.500000     71.500000   185.185000     3.500000     0.858056\n",
       "std       5.767283     41.575436   366.201103     1.708122     0.042975\n",
       "min       0.000000      0.000000     0.010000     1.000000     0.640000\n",
       "25%       4.750000     35.750000     0.100000     2.000000     0.860000\n",
       "50%       9.500000     71.500000     5.500000     3.500000     0.860000\n",
       "75%      14.250000    107.250000   100.000000     5.000000     0.880000\n",
       "max      19.000000    143.000000  1000.000000     6.000000     0.920000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='index.epoch'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[df['index.candid'] == 4].pivot(index='index.epoch', columns='index.candid', values='res.score').plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- Readme Doc\n",
    "- API Doc \n",
    "- File locking\n",
    "- Unit Test \n",
    "    - Multiloc with different aggregation layers (snowflake)\n",
    "    - File locking\n",
    "- Dev: Logging of edited files\n",
    "- Dev: File watcher\n",
    "- Dev: Excel File watcher / binder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
