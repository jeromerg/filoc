{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;\">\n",
    "  <span style=\"margin-top:auto; margin-bottom:auto; margin-right:0.5em;\"> Open notebook in binder: </span>\n",
    "  <span style=\"margin-top:auto; margin-bottom:auto;\"><a href=\"https://mybinder.org/v2/gh/jeromerg/filoc/master?filepath=examples%2Fexample_ml.ipynb\"><img src=\"https://mybinder.org/badge_logo.svg\" width=\"150\"></a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "### Improve rendering with custom style\n",
    "from IPython.display import HTML\n",
    "with open( './custom.css', 'r' ) as f:\n",
    "    display(HTML(f\"<style>{f.read()}</style>\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,os.path.abspath('../'))\n",
    "from filoc import filoc\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid Data Analysis\n",
    "\n",
    "This notebook loads the covid statistics from the [John Hopkins University Github repository](https://github.com/CSSEGISandData), clean them and visualize them dynamically.\n",
    "\n",
    "It illustrate, how the <span class=\"filoc\">filoc</span> framework enables to quickly read multiple files into a single DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load raw data\n",
    "\n",
    "The [John Hopkins University Github repository](https://github.com/CSSEGISandData) stores the daily historical data in separate CSV files. The <span class=\"filoc\">filoc</span> framework can read multiple file in almost any folder structure. And it has a pre-configured CSV Backend, so that you can read the whole data set by simply instantiating the following <span class=\"filoc\">filoc</span>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the filoc instance\n",
    "loc = filoc(\n",
    "    locpath='github://CSSEGISandData:COVID-19@/csse_covid_19_data/csse_covid_19_daily_reports/{date_str}.csv', \n",
    "    backend='csv', \n",
    "    encoding='utf-8-sig', \n",
    "    cache_locpath='~/temp/filoc_cache'  # disable this line to deactivate caching and allow to reload the data from the repository\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about the <span class=\"filoc\">filoc</span> parameters:\n",
    "- `locpath`: (required) Defines the location path of the files to read. Here with use the fsspec `github://` protocol to read the data directly from the github repository.\n",
    "- `backend`: (default 'json') Defines the backup instance, that must be used to read the files. <span class=\"filoc\">filoc</span> comes with four predefined backends: 'json', 'yaml', 'csv', 'pickle', but if you need, you can implement your own backend too.\n",
    "- `encoding`: (optional, default 'utf-8') Backend file encoding passed to the backend\n",
    "- `cache_locpath`: (optional, default None) Cache the backend loaded content to a local file, to speed up multiple reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading the whole data set, just check that the <span class=\"filoc\">filoc</span> works as expected, and that the expected files are loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check, whether filoc works properly\n",
    "loc.list_paths()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... everything is fine, we see that <span class=\"filoc\">filoc</span> fetched the file information from the github repository, we expect to download.\n",
    "\n",
    "Now we can download the data into a pandas DataFrame. This step can last a few minutes, depending of you internet connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = loc.read_contents()\n",
    "\n",
    "print(len(df_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole covid-19 statistics is loaded... now we can start the data cleaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make a copy... to enable \"try and error\" during the data cleaning work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the downloaded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV Backend does not perform any conversion, so all columns are loaded as string (in pandas `object` column type).\n",
    "\n",
    "After a \"try and error\" session, the column cleaning looks like as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic column cleaning\n",
    "df_all['country']        = df_all['Country/Region'].combine_first(df_all['Country_Region']).fillna(\"\").replace([\"None\"], \"\")\n",
    "df_all['province']       = df_all['Province/State'].combine_first(df_all['Province_State']).fillna(\"\").replace([\"None\"], \"\")\n",
    "df_all['date']           = pd.to_datetime(df_all['date_str'])\n",
    "df_all['confirmed']      = pd.to_numeric(df_all['Confirmed'], errors='coerce')\n",
    "df_all['recovered']      = pd.to_numeric(df_all['Recovered'], errors='coerce')\n",
    "df_all['deaths']         = pd.to_numeric(df_all['Deaths'], errors='coerce')\n",
    "\n",
    "df_all = df_all[['country', 'province', 'date', 'confirmed', 'recovered', 'deaths']].copy()\n",
    "df_all = df_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can re-check the columns after cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country / Province Cleaning\n",
    "\n",
    "The downloaded data are historical raw data and contains some variations and errors, that we need to fix. Again, it is an iterative \"try and error\" work. \n",
    "\n",
    "First we need to get adequate visualization, to get as much useful information in as few lines as possible.\n",
    "\n",
    "Here we try to get an insight for each country and each province:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build group-by (including helper function to rebuild the group-by sets)\n",
    "\n",
    "df_by_country          = None\n",
    "df_by_country_province = None\n",
    "\n",
    "def make_groups():\n",
    "    global df_by_country, df_by_country_province\n",
    "    df_by_country          = df_all.groupby(by=['country'])\n",
    "    df_by_country_province = df_all.groupby(by=['country', 'province'])\n",
    "\n",
    "make_groups()\n",
    "len(df_by_country_province)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check begin end of data\n",
    "df = df_by_country_province.agg([('Min' , 'min'), ('Max', 'max')])\n",
    "with pd.option_context(\"display.max_rows\", 2000):\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look into the long list, you will notice, some naming issues. As well as date range issues. For a few countries, the data granularity increased at some point in the past from country level data to province level data...\n",
    "\n",
    "So after a few \"try and error\" iterations, we get the following data cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special fixes based on date min-max explorations\n",
    "\n",
    "# Rename 'Hong Kong' country name to 'Hong Kong SAR'\n",
    "df_all.loc[ df_all['country'] == 'Hong Kong'     , 'country'] = 'Hong Kong SAR'\n",
    "# Rename 'Mainland China' country name to 'China'\n",
    "df_all.loc[ df_all['country'] == 'Mainland China', 'country'] = 'China'\n",
    "# Delete Country/Province: France/France \n",
    "df_all = df_all.drop(df_all.index[(df_all['country'] == 'France') & (df_all['province'] == 'France')])\n",
    "\n",
    "# For 'United Kingdom' Country, rename provinces 'UK' and '' to 'United Kingdom'\n",
    "# For 'Germany' Country, rename provinces 'Bavaria' to 'Bayern'\n",
    "# For 'France' Country, unnamed province '' is not the aggregate of province subdivisions, but the Mainland\n",
    "df_all.loc[ (df_all['country'] == 'United Kingdom') & df_all['province'].isin(['UK', '']), 'province'] = 'United Kingdom'\n",
    "df_all.loc[ (df_all['country'] == 'Germany'       ) & df_all['province'].isin(['Bavaria']), 'province'] = 'Bayern'\n",
    "df_all.loc[ (df_all['country'] == 'France'        ) & df_all['province'].isin([''])       , 'province'] = 'Mainland'\n",
    "\n",
    "# Rename all empty province '' by '<all>':\n",
    "df_all.loc[df_all['province'] == '', 'province'] = '<all>'\n",
    "\n",
    "# refresh group-bys\n",
    "make_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix country data, were data at country level were replaced by data at province level -> we now need to aggregate ourself the province data\n",
    "\n",
    "Fix = namedtuple(\"Fix\", [\"country\", \"last_aggr_date\"])\n",
    "fixes = [    \n",
    "    Fix('Australia'  , '2020-01-01'),\n",
    "    Fix('Brazil'     , '2020-05-19'),\n",
    "    Fix('Canada'     , '2010-01-01'),\n",
    "    Fix('Chile'      , '2020-05-19'),\n",
    "    Fix('China'      , '2010-01-01'),\n",
    "    Fix('Colombia'   , '2020-05-27'),\n",
    "    Fix('Germany'    , '2020-05-13'),\n",
    "    Fix('India'      , '2020-06-09'),\n",
    "    Fix('Italy'      , '2020-05-13'),\n",
    "    Fix('Japan'      , '2020-05-27'),\n",
    "    Fix('China'      , '2010-01-01'),\n",
    "    Fix('Mexico'     , '2020-05-19'),\n",
    "    Fix('Netherlands', '2020-07-16'),\n",
    "    Fix('Pakistan'   , '2020-06-09'),\n",
    "    Fix('Peru'       , '2020-05-27'),\n",
    "    Fix('Russia'     , '2020-05-31'),\n",
    "    Fix('Spain'      , '2020-05-13'),\n",
    "    Fix('Sweden'     , '2020-06-04'),\n",
    "    Fix('Ukraine'    , '2020-05-31'),    \n",
    "]\n",
    "\n",
    "for fix in fixes:\n",
    "    print(f'Fixing {fix.country}')\n",
    "    aggr_data_to_delete = df_all.index[  (df_all['country'] == fix.country) & (df_all['province'] == '<all>') & (df_all['date'] > fix.last_aggr_date)  ]\n",
    "    data_to_aggr        = df_all      [  (df_all['country'] == fix.country) & (df_all['province'] != '<all>') & (df_all['date'] > fix.last_aggr_date)  ]\n",
    "    df_all = df_all.drop(aggr_data_to_delete)\n",
    "\n",
    "    aggr_data_to_add = data_to_aggr.groupby('date').sum()\n",
    "    aggr_data_to_add['country'] = fix.country\n",
    "    aggr_data_to_add['province'] = '<all>'\n",
    "    aggr_data_to_add['date'] = aggr_data_to_add.index\n",
    "\n",
    "    df_all = pd.concat([df_all, aggr_data_to_add], ignore_index=True)\n",
    "\n",
    "make_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data by country / Province\n",
    "\n",
    "Just to illustrate an interesting feature of the <span class=\"filoc\">filoc</span> framework: you can easily save the cleaned data into an alternative file structure suiting your needs. Here for example, we save the historical data by country / province.\n",
    "\n",
    "First we instantiate a <span class=\"filoc\">filoc</span> with the expected path structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_save = filoc(\n",
    "    locpath='~/temp/covid/{country}/{province}/whole_history.csv', \n",
    "    backend='csv', \n",
    "    singleton=False, \n",
    "    writable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then just save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_save.write_contents(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the fist of files created by <span class=\"filoc\">filoc</span>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! find ~/temp/covid -type f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Data with matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot history for a single country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_province_filter = ('France' , 'Mainland',)\n",
    "indicators = [ 'confirmed', 'recovered', 'deaths' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "ax = plt.gca()\n",
    "df_cp = df_by_country_province.get_group(country_province_filter)\n",
    "df_cp.plot(x='date', y=indicators, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the 'confirmed' indicator for multiple countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator = 'confirmed'\n",
    "country_province_filter = [\n",
    "    ('France' , 'Metropole'),\n",
    "    ('Germany', '<all>'),\n",
    "    ('Spain'  , '<all>'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "ax = plt.gca()\n",
    "ax.set_title(indicator)\n",
    "for cp in country_province_filter:\n",
    "    label = f'{cp[0]} {cp[1]}'\n",
    "    df_cp = df_by_country_province.get_group(cp)\n",
    "    df_cp.plot(x='date', y=indicator, ax=ax, label=label);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a dash/plotly Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_dash import JupyterDash\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When running in JupyterHub or Binder, call the infer_jupyter_config function to detect the proxy configuration.\n",
    "JupyterDash.infer_jupyter_proxy_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def to_options(values, unique=True, sort=True):\n",
    "    \"\"\" helper dropdown options builder \"\"\"\n",
    "    if unique: values = values.unique()\n",
    "    if sort  : values = sorted(values)        \n",
    "    return [{'label': i, 'value': i} for i in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the app\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = JupyterDash('Covid 19 Stats', external_stylesheets=external_stylesheets)\n",
    "server = app.server\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            '''Choose a country:''',\n",
    "            dcc.Dropdown('country', to_options(df_all.country)),\n",
    "        ], style={'width': '49%', 'display': 'inline-block'}),\n",
    "        html.Div([\n",
    "            '''Choose a province:''',\n",
    "            dcc.Dropdown('province', disabled=True),\n",
    "        ], style={'width': '49%', 'float': 'right', 'display': 'inline-block'}),\n",
    "    ], style={\n",
    "        'borderBottom': 'thin lightgrey solid',\n",
    "        'backgroundColor': 'rgb(250, 250, 250)',\n",
    "        'padding': '10px 5px'}),\n",
    "    html.Div([\n",
    "        dcc.Graph('country-chart')\n",
    "    ], style={'width': '98%', 'display': 'inline-block', 'padding': '0 20'}),\n",
    "])\n",
    "\n",
    "@app.callback([\n",
    "    Output('province', 'disabled'),\n",
    "    Output('province', 'options'),\n",
    "], Input('country', 'value'))\n",
    "def update_province(country):\n",
    "    if country != '':\n",
    "        return False, to_options(df_all.province[df_all['country'] == country])\n",
    "    else:\n",
    "        return True, None\n",
    "        \n",
    "@app.callback(Output('country-chart', 'figure'), [Input('country', 'value'), Input('province', 'value')])\n",
    "def update_graph(country, province):\n",
    "    df = df_all[ (df_all['country'] == country) & (df_all['province'] == province) ]\n",
    "    indicator = 'deaths'\n",
    "    return {\n",
    "        'data': [ {\n",
    "            'x' : df['date'],\n",
    "            'y' : df['confirmed'],\n",
    "            'mode' : 'lines',\n",
    "            'name' : 'confirmed'\n",
    "        }, {\n",
    "            'x' : df['date'],\n",
    "            'y' : df['deaths'],\n",
    "            'mode' : 'lines',\n",
    "            'name' : 'deaths'\n",
    "        }],\n",
    "        'layout': {\n",
    "            'height': 500,\n",
    "            'margin': {'l': 30, 'b': 30, 'r': 10, 't': 10},\n",
    "            'annotations': [{\n",
    "                'x': 0, 'y': 0.85, 'xanchor': 'left', 'yanchor': 'bottom',\n",
    "                'xref': 'paper', 'yref': 'paper', 'showarrow': False,\n",
    "                'align': 'left', 'bgcolor': 'rgba(255, 255, 255, 0.5)',\n",
    "                'text': f'{country} / {province} / {indicator}'\n",
    "            }],\n",
    "            'yaxis': {'type': 'linear' },\n",
    "            'xaxis': {'showgrid': False}\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the app\n",
    "#app.run_server(mode=\"jupyterlab\")\n",
    "app.run_server(mode=\"inline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
